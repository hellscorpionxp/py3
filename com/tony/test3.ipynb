{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.6, 1.4, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}\n",
      "[[4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [6.7 3.  5.  1.7]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  2.  3.5 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [7.7 3.8 6.7 2.2]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as ds\n",
    "import sklearn.model_selection as ms\n",
    "\n",
    "iris = ds.load_iris()\n",
    "print(type(iris))\n",
    "print(iris)\n",
    "\n",
    "x_train, x_test, y_train, y_test = ms.train_test_split(iris.data, iris.target, test_size=0.2, random_state=22)\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1.0\n",
      "  (0, 3)\t100.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 3)\t60.0\n",
      "  (2, 2)\t1.0\n",
      "  (2, 3)\t30.0\n",
      "['city=上海', 'city=北京', 'city=深圳', 'temp']\n",
      "[[  0.   1.   0. 100.]\n",
      " [  1.   0.   0.  60.]\n",
      " [  0.   0.   1.  30.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "data = [{'city': '北京', 'temp':100}, {'city': '上海', 'temp':60}, {'city': '深圳', 'temp':30}]\n",
    "transfer = DictVectorizer()\n",
    "data_sparse = transfer.fit_transform(data)\n",
    "print(data_sparse)\n",
    "transfer = DictVectorizer(sparse=False)\n",
    "data_onehot = transfer.fit_transform(data)\n",
    "print(transfer.get_feature_names())\n",
    "print(data_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 3)\t2\n",
      "  (0, 5)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 0)\t1\n",
      "['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too']\n",
      "[[0 1 1 2 0 1 1 0]\n",
      " [1 1 1 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = [\"life is short, i like like python\", \"life is too long, i dislike python\"]\n",
    "transfer = CountVectorizer()\n",
    "data_sparse = transfer.fit_transform(data)\n",
    "print(type(data_sparse))\n",
    "print(data_sparse)\n",
    "print(transfer.get_feature_names())\n",
    "print(data_sparse.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 13)\t0.5773502691896257\n",
      "  (0, 15)\t0.5773502691896257\n",
      "  (0, 0)\t0.5773502691896257\n",
      "  (1, 18)\t0.2938838601653297\n",
      "  (1, 12)\t0.2938838601653297\n",
      "  (1, 6)\t0.2938838601653297\n",
      "  (1, 21)\t0.2938838601653297\n",
      "  (1, 16)\t0.2938838601653297\n",
      "  (1, 25)\t0.2938838601653297\n",
      "  (1, 14)\t0.2938838601653297\n",
      "  (1, 24)\t0.2235062506454222\n",
      "  (1, 27)\t0.2938838601653297\n",
      "  (1, 3)\t0.2938838601653297\n",
      "  (1, 8)\t0.2938838601653297\n",
      "  (1, 1)\t0.2938838601653297\n",
      "  (2, 26)\t0.2385120556095375\n",
      "  (2, 22)\t0.2385120556095375\n",
      "  (2, 28)\t0.2385120556095375\n",
      "  (2, 19)\t0.2385120556095375\n",
      "  (2, 11)\t0.2385120556095375\n",
      "  (2, 7)\t0.2385120556095375\n",
      "  (2, 9)\t0.2385120556095375\n",
      "  (2, 5)\t0.2385120556095375\n",
      "  (2, 23)\t0.2385120556095375\n",
      "  (2, 4)\t0.2385120556095375\n",
      "  (2, 10)\t0.2385120556095375\n",
      "  (2, 20)\t0.2385120556095375\n",
      "  (2, 29)\t0.2385120556095375\n",
      "  (2, 31)\t0.2385120556095375\n",
      "  (2, 30)\t0.2385120556095375\n",
      "  (2, 17)\t0.2385120556095375\n",
      "  (2, 2)\t0.2385120556095375\n",
      "  (2, 24)\t0.18139456604738435\n",
      "['1898', '一个', '三三两两', '上午', '两位', '京戏', '兴致勃勃', '几个', '初秋', '另外', '喝茶', '围着', '坐在', '失败', '开始', '戊戌变法', '掌柜', '旗人', '柜台', '桌子', '歇腿', '王利发', '瓦罐', '茶客', '茶馆', '营业', '蟋蟀', '裕泰', '观赏', '走进', '遛够', '鸟儿']\n",
      "[[0.57735027 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.57735027 0.         0.57735027 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.29388386 0.         0.29388386 0.         0.\n",
      "  0.29388386 0.         0.29388386 0.         0.         0.\n",
      "  0.29388386 0.         0.29388386 0.         0.29388386 0.\n",
      "  0.29388386 0.         0.         0.29388386 0.         0.\n",
      "  0.22350625 0.29388386 0.         0.29388386 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.23851206 0.         0.23851206 0.23851206\n",
      "  0.         0.23851206 0.         0.23851206 0.23851206 0.23851206\n",
      "  0.         0.         0.         0.         0.         0.23851206\n",
      "  0.         0.23851206 0.23851206 0.         0.23851206 0.23851206\n",
      "  0.18139457 0.         0.23851206 0.         0.23851206 0.23851206\n",
      "  0.23851206 0.23851206]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba\n",
    "\n",
    "data = [\"1898年，戊戌变法失败。\",\n",
    "        \"一个初秋的上午，裕泰茶馆开始营业，掌柜王利发兴致勃勃地坐在柜台上。\",\n",
    "        \"三三两两的旗人，遛够了鸟儿，走进茶馆来歇腿、喝茶。有两位茶客唱着京戏，另外几个围着桌子观赏瓦罐中的蟋蟀。\"]\n",
    "data_segment = []\n",
    "for text in data:\n",
    "    data_segment.append(\" \".join(list(jieba.cut(text))))\n",
    "transfer = TfidfVectorizer()\n",
    "data_sparse = transfer.fit_transform(data_segment)\n",
    "print(type(data_sparse))\n",
    "print(data_sparse)\n",
    "print(transfer.get_feature_names())\n",
    "print(data_sparse.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0         1         2\n",
      "0  40920  8.326976  0.953952\n",
      "1  14488  7.153469  1.673904\n",
      "2  26052  1.441871  0.805124\n",
      "[[1.         1.         0.17130689]\n",
      " [0.         0.82955859 1.        ]\n",
      " [0.4375     0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas\n",
    "\n",
    "data = [[40920, 8.326976, 0.953952, 3],\n",
    "       [14488, 7.153469, 1.673904, 2],\n",
    "       [26052, 1.441871, 0.805124, 1]]\n",
    "data_sub = pandas.DataFrame(data).iloc[:, :3]\n",
    "print(data_sub)\n",
    "transfer = MinMaxScaler(feature_range=[0, 1])\n",
    "data_norm = transfer.fit_transform(data_sub)\n",
    "print(data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0         1         2\n",
      "0  40920  8.326976  0.953952\n",
      "1  14488  7.153469  1.673904\n",
      "2  26052  1.441871  0.805124\n",
      "[[ 1.2724665   0.8931759  -0.50182471]\n",
      " [-1.17066918  0.50297911  1.39595776]\n",
      " [-0.10179732 -1.39615501 -0.89413304]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas\n",
    "\n",
    "data = [[40920, 8.326976, 0.953952, 3],\n",
    "       [14488, 7.153469, 1.673904, 2],\n",
    "       [26052, 1.441871, 0.805124, 1]]\n",
    "data_sub = pandas.DataFrame(data).iloc[:, :3]\n",
    "print(data_sub)\n",
    "transfer = StandardScaler()\n",
    "data_norm = transfer.fit_transform(data_sub)\n",
    "print(data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pe_ratio  pb_ratio    market_cap  return_on_asset_net_profit  \\\n",
      "0    5.9572    1.1818  8.525255e+10                      0.8008   \n",
      "1    7.0289    1.5880  8.411336e+10                      1.6463   \n",
      "2 -262.7461    7.0003  5.170455e+08                     -0.5678   \n",
      "\n",
      "   du_return_on_equity            ev  earnings_per_share       revenue  \\\n",
      "0              14.9403  1.211445e+12               2.010  2.070140e+10   \n",
      "1               7.8656  3.002521e+11               0.326  2.930837e+10   \n",
      "2              -0.5943  7.705178e+08              -0.006  1.167983e+07   \n",
      "\n",
      "   total_expense  \n",
      "0   1.088254e+10  \n",
      "1   2.378348e+10  \n",
      "2   1.203008e+07  \n",
      "[[ 5.95720000e+00  1.18180000e+00  8.52525509e+10  1.49403000e+01\n",
      "   1.21144486e+12  2.07014010e+10  1.08825400e+10]\n",
      " [ 7.02890000e+00  1.58800000e+00  8.41133582e+10  7.86560000e+00\n",
      "   3.00252062e+11  2.93083692e+10  2.37834769e+10]\n",
      " [-2.62746100e+02  7.00030000e+00  5.17045520e+08 -5.94300000e-01\n",
      "   7.70517753e+08  1.16798290e+07  1.20300800e+07]] (3, 7)\n",
      "(-0.9978220703451082, 0.04202385587723676)\n",
      "(0.9602119805026594, 0.18018634622785798)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "data = pd.read_csv(\"./datasets/factor_returns.csv\")\n",
    "data = data.iloc[:, 1:-2]\n",
    "print(data)\n",
    "\n",
    "transfer = VarianceThreshold(threshold=5)\n",
    "data_variance = transfer.fit_transform(data)\n",
    "print(data_variance, data_variance.shape)\n",
    "\n",
    "r1 = pearsonr(data[\"pe_ratio\"], data[\"pb_ratio\"])\n",
    "r2 = pearsonr(data[\"revenue\"], data[\"total_expense\"])\n",
    "print(r1)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.22879107e-15  3.82970843e+00]\n",
      " [ 5.74456265e+00 -1.91485422e+00]\n",
      " [-5.74456265e+00 -1.91485422e+00]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = [[2,8,4,5], [6,3,0,8], [5,4,9,1]]\n",
    "transfer = PCA(n_components=0.95)\n",
    "data_pca = transfer.fit_transform(data)\n",
    "print(data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 44)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "order_products = pd.read_csv(\"./instacart/order_products_prior.csv\")\n",
    "products = pd.read_csv(\"./instacart/products.csv\")\n",
    "orders = pd.read_csv(\"./instacart/orders.csv\")\n",
    "aisles = pd.read_csv(\"./instacart/aisles.csv\")\n",
    "\n",
    "tab1 = pd.merge(aisles, products, on=[\"aisle_id\", \"aisle_id\"])\n",
    "# print(tab1)\n",
    "tab2 = pd.merge(tab1, order_products, on=[\"product_id\", \"product_id\"])\n",
    "tab3 = pd.merge(tab2, orders, on=[\"order_id\", \"order_id\"])\n",
    "# print(tab3)\n",
    "tab4 = pd.crosstab(tab3[\"user_id\"], tab3[\"aisle\"])\n",
    "tab4\n",
    "\n",
    "data = tab4[:100000]\n",
    "transfer = PCA(n_components=0.95)\n",
    "data_pca = transfer.fit_transform(data)\n",
    "data_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
